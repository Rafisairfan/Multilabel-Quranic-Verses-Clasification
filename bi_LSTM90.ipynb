{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcyZl1S4T13C"
   },
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CIXntef2V_r0",
    "outputId": "dd8e62fc-b563-4e9c-c0fe-ac3897956300"
   },
   "outputs": [],
   "source": [
    "#basic library ===============================================================#=\n",
    "import pandas as pd                                                           \n",
    "import numpy as np                                                                                                            \n",
    "import re                                                                     \n",
    "import joblib                                                                 \n",
    "import string                                                                 \n",
    "#NLTK Corner =================================================================#=\n",
    "import nltk                                                                    \n",
    "from nltk.corpus import stopwords                                             \n",
    "from nltk.tokenize import word_tokenize                                       \n",
    "from nltk.stem import PorterStemmer                                           \n",
    "from nltk.tokenize import word_tokenize                                                                                     \n",
    "#Tensorflow Corner ===========================================================#=\n",
    "import tensorflow as tf\n",
    "from sklearn import model_selection \n",
    "from sklearn import preprocessing                                                      \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer                     \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences             \n",
    "#Sklearn gang                                                                 \n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer                   \n",
    "from sklearn.model_selection import train_test_split                          \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler                   \n",
    "#Keras club ==================================================================#=\n",
    "from keras.models import Sequential                                           \n",
    "from keras import layers                                                      \n",
    "from tensorflow import keras\n",
    "from keras.preprocessing import sequence                                      \n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, SpatialDropout1D,Bidirectional,MaxPool1D, Flatten\n",
    "import tensorflow_addons as tfa\n",
    "#Word2vec\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "d027gjxOWEW9"
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv('X.csv')\n",
    "Y = pd.read_csv('Y.csv')\n",
    "Y = Y.drop(\"Unnamed: 0\", axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TrS5C5waQvb"
   },
   "source": [
    "## Modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4jlGN7RL5ANe"
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 256\n",
    "MAX_SEQUENCE_LENGTH = 150 \n",
    "EMBEDDING_DIM = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CBjr1_Xp50Bl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3497 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)\n",
    "tokenizer.fit_on_texts(X)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ccNg17ze5-kg",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (6236, 150)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCgSevPlhycy"
   },
   "source": [
    "## Splitting Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rsUgjI2khwv8",
    "outputId": "b295cb12-2875-478b-ef58-eb6b3f8298c5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(16, activation='softmax'))\n",
    "model.compile(loss=tf.keras.losses.BinaryFocalCrossentropy(), optimizer = tf.keras.optimizers.Adam(learning_rate=0.001), metrics=tf.keras.metrics.top_k_categorical_accuracy) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Xr4HJigwNsaX",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "176/176 [==============================] - 78s 390ms/step - loss: 0.0220 - top_k_categorical_accuracy: 0.7060 - val_loss: 0.0144 - val_top_k_categorical_accuracy: 0.8734\n",
      "Epoch 2/10\n",
      "176/176 [==============================] - 63s 360ms/step - loss: 0.0159 - top_k_categorical_accuracy: 0.7557 - val_loss: 0.0143 - val_top_k_categorical_accuracy: 0.8894\n",
      "Epoch 3/10\n",
      "176/176 [==============================] - 62s 353ms/step - loss: 0.0151 - top_k_categorical_accuracy: 0.8002 - val_loss: 0.0140 - val_top_k_categorical_accuracy: 0.8638\n",
      "Epoch 4/10\n",
      "176/176 [==============================] - 63s 356ms/step - loss: 0.0146 - top_k_categorical_accuracy: 0.8421 - val_loss: 0.0139 - val_top_k_categorical_accuracy: 0.8734\n",
      "Epoch 5/10\n",
      "176/176 [==============================] - 62s 351ms/step - loss: 0.0145 - top_k_categorical_accuracy: 0.8592 - val_loss: 0.0139 - val_top_k_categorical_accuracy: 0.8766\n",
      "Epoch 6/10\n",
      "176/176 [==============================] - 59s 334ms/step - loss: 0.0143 - top_k_categorical_accuracy: 0.8713 - val_loss: 0.0138 - val_top_k_categorical_accuracy: 0.8766\n",
      "Epoch 7/10\n",
      "176/176 [==============================] - 60s 344ms/step - loss: 0.0141 - top_k_categorical_accuracy: 0.8735 - val_loss: 0.0137 - val_top_k_categorical_accuracy: 0.8830\n",
      "Epoch 8/10\n",
      "176/176 [==============================] - 61s 347ms/step - loss: 0.0141 - top_k_categorical_accuracy: 0.8820 - val_loss: 0.0137 - val_top_k_categorical_accuracy: 0.8494\n",
      "Epoch 9/10\n",
      "176/176 [==============================] - 63s 361ms/step - loss: 0.0140 - top_k_categorical_accuracy: 0.8783 - val_loss: 0.0137 - val_top_k_categorical_accuracy: 0.8878\n",
      "Epoch 10/10\n",
      "176/176 [==============================] - 69s 394ms/step - loss: 0.0137 - top_k_categorical_accuracy: 0.8799 - val_loss: 0.0139 - val_top_k_categorical_accuracy: 0.8766\n",
      "20/20 [==============================] - 5s 119ms/step\n",
      "20/20 [==============================] - 2s 116ms/step - loss: 0.0139 - top_k_categorical_accuracy: 0.8766\n",
      "Epoch 1/10\n",
      "176/176 [==============================] - 72s 410ms/step - loss: 0.0136 - top_k_categorical_accuracy: 0.8803 - val_loss: 0.0138 - val_top_k_categorical_accuracy: 0.8830\n",
      "Epoch 2/10\n",
      "176/176 [==============================] - 70s 397ms/step - loss: 0.0135 - top_k_categorical_accuracy: 0.8842 - val_loss: 0.0146 - val_top_k_categorical_accuracy: 0.8526\n",
      "Epoch 3/10\n",
      "176/176 [==============================] - 72s 408ms/step - loss: 0.0135 - top_k_categorical_accuracy: 0.8868 - val_loss: 0.0141 - val_top_k_categorical_accuracy: 0.8381\n",
      "Epoch 4/10\n",
      "176/176 [==============================] - 69s 389ms/step - loss: 0.0133 - top_k_categorical_accuracy: 0.8877 - val_loss: 0.0154 - val_top_k_categorical_accuracy: 0.8590\n",
      "Epoch 5/10\n",
      "176/176 [==============================] - 73s 413ms/step - loss: 0.0133 - top_k_categorical_accuracy: 0.8863 - val_loss: 0.0140 - val_top_k_categorical_accuracy: 0.8798\n",
      "Epoch 6/10\n",
      "176/176 [==============================] - 74s 420ms/step - loss: 0.0131 - top_k_categorical_accuracy: 0.8879 - val_loss: 0.0168 - val_top_k_categorical_accuracy: 0.8766\n",
      "Epoch 7/10\n",
      "176/176 [==============================] - 73s 413ms/step - loss: 0.0130 - top_k_categorical_accuracy: 0.8947 - val_loss: 0.0148 - val_top_k_categorical_accuracy: 0.8606\n",
      "Epoch 8/10\n",
      "176/176 [==============================] - 66s 375ms/step - loss: 0.0131 - top_k_categorical_accuracy: 0.8868 - val_loss: 0.0161 - val_top_k_categorical_accuracy: 0.8798\n",
      "Epoch 9/10\n",
      "176/176 [==============================] - 61s 346ms/step - loss: 0.0128 - top_k_categorical_accuracy: 0.8922 - val_loss: 0.0174 - val_top_k_categorical_accuracy: 0.8718\n",
      "Epoch 10/10\n",
      "176/176 [==============================] - 58s 327ms/step - loss: 0.0126 - top_k_categorical_accuracy: 0.8888 - val_loss: 0.0161 - val_top_k_categorical_accuracy: 0.8814\n",
      "20/20 [==============================] - 2s 111ms/step\n",
      "20/20 [==============================] - 2s 119ms/step - loss: 0.0161 - top_k_categorical_accuracy: 0.8814\n",
      "[0.8766025900840759, 0.8814102411270142]\n",
      "[0.102163464, 0.1030649]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "acc = []; loss= []\n",
    "for item in range(2) :\n",
    "    history=model.fit(X_train, Y_train,verbose = 1,\n",
    "               batch_size=32,\n",
    "               epochs=10,\n",
    "               validation_data=[X_test, Y_test])\n",
    "    item = item + 1\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = (model.evaluate(X_test, Y_test, batch_size=32, verbose=1))\n",
    "    acc.append(score[1])\n",
    "    metric = tfa.metrics.HammingLoss(mode='multilabel', threshold=0.1)\n",
    "    metric.update_state(Y_test, y_pred)\n",
    "    loss.append(metric.result().numpy())\n",
    "\n",
    "    \n",
    "print(acc)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1041666641831398"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history.save('F_biLSTM_256_128_64_10_a8914_l0.10101162.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KmGHs5s2KL1"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "uw-mm1La2LiY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 2s 96ms/step\n",
      "Validation accuracy: 0.8894230723381042\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "score = model.evaluate(X_test, Y_test, batch_size=32, verbose=0)\n",
    "print('Validation accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Hyn2pJZv1tas"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  0.104467146\n"
     ]
    }
   ],
   "source": [
    "metric = tfa.metrics.HammingLoss(mode='multilabel', threshold=0.1)\n",
    "metric.update_state(Y_test, y_pred)\n",
    "print(\"loss = \", metric.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02083333283662796"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
